{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iceberg PII Data Deletion Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook walks through the process of creating an Iceberg table, adding data, deleting PII, and then permanently removing the history containing the PII.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to import pyspark and set up our Spark session. The configuration for the S3 endpoint and Iceberg catalog is already handled by the `docker-compose.yml` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IcebergPIIDemo\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create the Iceberg Table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll create an Iceberg table called `pii_data` in our `demo` catalog. The schema will include the PII columns we want to manage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS demo.pii_data (\n",
        "    case_id STRING,\n",
        "    first_name STRING,\n",
        "    email_address STRING,\n",
        "    key_nm STRING,\n",
        "    secure_txt STRING,\n",
        "    secure_key STRING,\n",
        "    update_date DATE\n",
        ")\n",
        "USING iceberg\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Seed the Table with Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's insert some sample data into our table. We'll add two records, one of which we will target for PII deletion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "INSERT INTO demo.pii_data VALUES\n",
        "('case-1', 'John', 'john.doe@example.com', 'key1', 'secret text 1', 'secret_key_1', '2023-01-01'),\n",
        "('case-2', 'Jane', 'jane.doe@example.com', 'key2', 'secret text 2', 'secret_key_2', '2023-01-02')\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify the data is there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.table(\"demo.pii_data\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also inspect the table's history to see the snapshot that was created when we inserted the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "initial_snapshots = spark.table(\"demo.pii_data.history\")\n",
        "initial_snapshots.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Delete PII\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will \"delete\" the PII for `case-1`. In this context, \"deletion\" means updating the PII columns to `NULL`. This is a common strategy for retaining the record for referential integrity while removing the sensitive information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_pii(case_id):\n",
        "    spark.sql(f\"\"\"\n",
        "    UPDATE demo.pii_data\n",
        "    SET\n",
        "        first_name = NULL,\n",
        "        email_address = NULL,\n",
        "        secure_txt = NULL\n",
        "    WHERE case_id = '{case_id}'\n",
        "    \"\"\")\n",
        "\n",
        "delete_pii('case-1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the data again. We should see that the PII for `case-1` is now gone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.table(\"demo.pii_data\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we look at the table history, we'll see a new snapshot has been added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.table(\"demo.pii_data.history\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Problem: Time Travel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even though we've \"deleted\" the PII from the current view of the table, the old data still exists in the previous snapshot. Anyone with access can use time travel to see the PII.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_snapshot_id = initial_snapshots.select(\"snapshot_id\").first()[0]\n",
        "spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.pii_data\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Permanent Deletion with Maintenance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To permanently remove the PII, we need to perform two maintenance operations:\n",
        "1.  **Expire Snapshots**: This removes old snapshots from the table's metadata, making time travel to those versions impossible.\n",
        "2.  **Rewrite Data Files (VACUUM)**: This physically rewrites the data files to remove data that is no longer referenced by any snapshot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expire Old Snapshots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll expire all snapshots that are older than the current one. We can get the current timestamp and use that to expire anything older.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
        "spark.sql(f\"CALL demo.system.expire_snapshots('pii_data', TIMESTAMP '{now}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, if we look at the history, we should only see the most recent snapshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.table(\"demo.pii_data.history\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rewrite Data Files (VACUUM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even though the snapshots are gone, the underlying Parquet files containing the PII may still exist in S3. The `rewrite_data_files` procedure (similar to VACUUM in other systems) will consolidate data into new files and remove the old, unreferenced ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"CALL demo.system.rewrite_data_files('pii_data')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's try to time travel back to the first snapshot. This should fail because the snapshot no longer exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.pii_data\").show()\n",
        "except Exception as e:\n",
        "    print(\"Successfully prevented time travel!\")\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This confirms that we have successfully and permanently deleted the PII from our Iceberg table.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
