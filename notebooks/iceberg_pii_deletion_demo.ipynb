{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Iceberg PII Data Deletion Demo\n",
        "\n",
        "This notebook demonstrates how to **permanently delete PII (Personally Identifiable Information)** from Apache Iceberg tables using a complete data lifecycle approach.\n",
        "\n",
        "# What You'll Learn\n",
        "\n",
        "1. **The Problem**: Why simple SQL `DELETE` isn't enough for PII removal in Iceberg\n",
        "2. **The Solution**: A complete data lifecycle approach using Iceberg maintenance operations\n",
        "3. **Key Operations**: \n",
        "   - Logical deletion (setting PII to NULL)\n",
        "   - Snapshot expiration (removing time travel access)\n",
        "   - Orphaned file cleanup (removing unreferenced files)\n",
        "   - Data file rewriting (physical PII removal)\n",
        "\n",
        "# Why This Matters\n",
        "\n",
        "Iceberg's time travel feature means that even after \"deleting\" data, it can still be accessed through historical snapshots. For PII compliance (GDPR, CCPA, etc.), you need to ensure data is **permanently and irreversibly** removed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Setup and Configuration\n",
        "\n",
        "First, let's set up our Spark session and import the utility functions we'll need for this demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all utility functions using the import script\n",
        "exec(open('import_utils.py').read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Spark for S3A and MinIO\n",
        "print(\"Spark:\", spark.version)\n",
        "hconf = spark._jsc.hadoopConfiguration()\n",
        "\n",
        "# S3A configuration for MinIO\n",
        "hconf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "hconf.set(\"fs.AbstractFileSystem.s3.impl\", \"org.apache.hadoop.fs.s3a.S3A\")\n",
        "hconf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
        "hconf.set(\"fs.s3a.path.style.access\", \"true\")\n",
        "hconf.set(\"fs.s3a.access.key\", \"admin\")\n",
        "hconf.set(\"fs.s3a.secret.key\", \"password\")\n",
        "hconf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "hconf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
        "\n",
        "# Verify S3A is working\n",
        "spark._jvm.java.lang.Class.forName(\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
        "    spark._jvm.java.net.URI.create(\"s3://warehouse\"),\n",
        "    spark._jsc.hadoopConfiguration()\n",
        ")\n",
        "print(\"✅ S3A configuration successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Iceberg catalog configuration\n",
        "print(\"Iceberg Catalog Configuration:\")\n",
        "for k, v in spark.sparkContext.getConf().getAll():\n",
        "    if \"catalog\" in k:\n",
        "        print(f\"  {k} = {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Create the PII Data Table\n",
        "\n",
        "Let's create an Iceberg table to store PII data. We'll include various types of sensitive information that we'll later need to delete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up any existing table via REST API\n",
        "print(\"=== Cleaning up any existing table via REST API ===\")\n",
        "!curl -X DELETE http://rest:8181/v1/namespaces/default/tables/pii_data\n",
        "\n",
        "# Also clean up via SQL as backup\n",
        "spark.sql(\"DROP TABLE IF EXISTS demo.default.pii_data\")\n",
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.default\")\n",
        "\n",
        "# Create the PII data table\n",
        "spark.sql(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS demo.default.pii_data (\n",
        "    case_id STRING,\n",
        "    first_name STRING,\n",
        "    email_address STRING,\n",
        "    key_nm STRING,\n",
        "    secure_txt STRING,\n",
        "    secure_key STRING,\n",
        "    update_date DATE\n",
        ")\n",
        "USING iceberg\n",
        "\"\"\")\n",
        "\n",
        "print(\"✅ PII data table created successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the base path for our table for easy reuse\n",
        "table_base_path = \"s3a://warehouse/default/pii_data\"\n",
        "\n",
        "# Check initial state\n",
        "_, _, all_previous = summarize_files(spark, table_base_path, \"After Table Creation\")\n",
        "print(\"Initial file state:\")\n",
        "all_previous\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Insert Sample PII Data\n",
        "\n",
        "Now let's add some sample data containing PII that we'll later need to delete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert sample PII data\n",
        "spark.sql(\"\"\"\n",
        "INSERT INTO demo.default.pii_data VALUES\n",
        "('case-1', 'John', 'john.doe@example.com', 'key1', 'secret text 1', 'secret_key_1', DATE('2023-01-01')),\n",
        "('case-2', 'Jane', 'jane.doe@example.com', 'key2', 'secret text 2', 'secret_key_2', DATE('2023-01-02')),\n",
        "('case-3', 'Alice', 'alice.smith@example.com', 'key3', 'secret text 3', 'secret_key_3', DATE('2023-01-03'))\n",
        "\"\"\")\n",
        "\n",
        "print(\"✅ Sample PII data inserted\")\n",
        "print(\"\\nCurrent table data:\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the table history (snapshots)\n",
        "print(\"Table snapshots:\")\n",
        "initial_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
        "initial_snapshots.show(truncate=False)\n",
        "\n",
        "# Check files after data insertion\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After Data Insertion\")\n",
        "print(\"\\nFile summary after data insertion:\")\n",
        "all_current\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Create Orphaned Files (Simulation)\n",
        "\n",
        "Let's create some orphaned files to demonstrate cleanup later. These files exist in S3 but aren't tracked by Iceberg metadata, simulating failed writes or manual operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files before creating orphaned files\n",
        "_, _, all_previous = summarize_files(spark, table_base_path, \"Before Creating Orphaned Files\")\n",
        "\n",
        "# Create orphaned files to demonstrate cleanup\n",
        "create_orphaned_files(spark)\n",
        "\n",
        "# Show all files in the data directory (including orphaned ones)\n",
        "print(\"All files under data/ directory (including orphaned):\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after creating orphaned files\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After Creating Orphaned Files\")\n",
        "print(\"\\nFile summary after creating orphaned files:\")\n",
        "all_current\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (Orphaned Files Added) ===\")\n",
        "diff_orphaned = diff_summaries(all_previous, all_current)\n",
        "diff_orphaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4b. Create Merge-on-Read Delete Files (simulate previous deletion)\n",
        "\n",
        "For MOR, `DELETE` does not rewrite data files immediately. Instead, it produces\n",
        "*delete files* that mask rows at read time. We'll:\n",
        "1) Ensure the table is configured for MOR deletes\n",
        "2) Issue a DELETE (simulating a previous deletion operation)\n",
        "3) Inspect metadata to confirm delete files exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Configure table for MOR deletes ===\")\n",
        "spark.sql(\"\"\"\n",
        "  ALTER TABLE demo.default.pii_data SET TBLPROPERTIES (\n",
        "    'write.delete.mode' = 'merge-on-read',\n",
        "    'write.update.mode' = 'copy-on-write' -- keep updates as COW for this demo\n",
        "  )\n",
        "\"\"\")\n",
        "print(\"✅ Table configured for MOR deletes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Create MOR delete files via SQL DELETE ===\")\n",
        "# Delete the 'case-2' record entirely (simulating a previous deletion operation)\n",
        "spark.sql(\"\"\"\n",
        "  DELETE FROM demo.default.pii_data\n",
        "  WHERE case_id = 'case-2'\n",
        "\"\"\")\n",
        "print(\"✅ DELETE issued (MOR): row is gone from reads, but data still in files until rewrite\")\n",
        "\n",
        "# Show current table (case-2 should no longer appear)\n",
        "print(\"\\nCurrent table data (case-2 should be gone):\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after MOR delete\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After MOR Delete\")\n",
        "print(\"\\nFile summary after MOR delete:\")\n",
        "all_current\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (MOR Delete) ===\")\n",
        "diff_mor = diff_summaries(all_previous, all_current)\n",
        "diff_mor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect metadata to verify delete files were written\n",
        "print(\"=== Inspect metadata: delete files present? ===\")\n",
        "# Iceberg metadata: 'files' table includes both data and delete files.\n",
        "# content: 0=data, 1=position deletes, 2=equality deletes\n",
        "files_meta = spark.table(\"demo.default.pii_data.files\")\n",
        "print(\"All file contents (content=0 data, 1 pos-delete, 2 eq-delete):\")\n",
        "files_meta.select(\"content\",\"file_path\",\"record_count\",\"file_format\").show(truncate=False)\n",
        "\n",
        "print(\"Delete files only (content IN (1,2)):\")\n",
        "files_meta.filter(\"content IN (1,2)\") \\\n",
        "          .select(\"content\",\"file_path\",\"record_count\",\"file_format\") \\\n",
        "          .show(truncate=False)\n",
        "\n",
        "# Optional: keep a count for validation before/after rewrite\n",
        "delete_file_count_before = files_meta.filter(\"content IN (1,2)\").count()\n",
        "print(f\"Delete files BEFORE rewrite_data_files: {delete_file_count_before}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. The Problem: Logical Deletion Isn't Enough\n",
        "\n",
        "Now let's demonstrate the core problem. We'll \"delete\" PII by setting it to NULL, but this doesn't actually remove the data permanently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Logically \"delete\" PII by setting it to NULL\n",
        "print(\"=== Step 1: Logical PII Deletion ===\")\n",
        "\n",
        "# Execute logical deletion by setting PII columns to NULL\n",
        "spark.sql(\"\"\"\n",
        "UPDATE demo.default.pii_data\n",
        "SET\n",
        "    first_name = NULL,\n",
        "    email_address = NULL,\n",
        "    secure_txt = NULL\n",
        "WHERE case_id = 'case-1'\n",
        "\"\"\")\n",
        "\n",
        "print(\"Data after logical deletion:\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after logical deletion\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After Logical Deletion\")\n",
        "print(\"\\nFile summary after logical deletion:\")\n",
        "all_current\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (Logical Deletion) ===\")\n",
        "diff_logical = diff_summaries(all_previous, all_current)\n",
        "diff_logical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Show that the PII still exists in previous snapshots!\n",
        "print(\"=== Step 2: The Problem - Time Travel ===\")\n",
        "print(\"Even though we 'deleted' the PII, it still exists in previous snapshots:\")\n",
        "\n",
        "# Get the first snapshot ID\n",
        "first_snapshot_id = initial_snapshots.select(\"snapshot_id\").first()[0]\n",
        "print(f\"\\nTime traveling back to snapshot {first_snapshot_id}:\")\n",
        "df_time_travel = spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").toPandas()\n",
        "display(df_time_travel)\n",
        "\n",
        "print(\"\\n🚨 PROBLEM: The PII is still accessible through time travel!\")\n",
        "print(\"This violates data privacy regulations like GDPR and CCPA.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's also examine any delete files that might exist\n",
        "print(\"\\n=== Examining Delete Files (if any) ===\")\n",
        "examine_delete_files(spark)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. The Solution: Complete PII Lifecycle Management\n",
        "\n",
        "To permanently delete PII, we need to perform a series of Iceberg maintenance operations. Let's walk through each step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Expire Old Snapshots\n",
        "\n",
        "First, we expire old snapshots to remove time travel access to the PII data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files before expiring snapshots\n",
        "print(\"=== Before Expiring Snapshots ===\")\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"Before Expiring Snapshots\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expires snapshots: CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '2024-09-10 20:00:00')\n",
        "\n",
        "# Expire all snapshots older than current timestamp\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
        "print(f\"Expiring snapshots older than: {now}\")\n",
        "\n",
        "spark.sql(f\"CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '{now}')\")\n",
        "print(\"✅ Old snapshots expired\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that time travel no longer works\n",
        "print(\"=== Verification: Time Travel Blocked ===\")\n",
        "try:\n",
        "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n",
        "except Exception as e:\n",
        "    print(\"✅ SUCCESS: Time travel to old snapshots is now blocked!\")\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Check current table state\n",
        "print(\"\\nCurrent table state:\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n",
        "\n",
        "print(\"\\nTable history (should only have current snapshot):\")\n",
        "df_history = spark.table(\"demo.default.pii_data.history\").toPandas()\n",
        "display(df_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after snapshot expiration\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After Snapshot Expiration\")\n",
        "print(\"\\nFile summary after snapshot expiration:\")\n",
        "all_current\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (Snapshot Expiration) ===\")\n",
        "diff_snapshots = diff_summaries(all_previous, all_current)\n",
        "diff_snapshots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean Up Orphaned Files\n",
        "\n",
        "Now let's clean up the orphaned files we created earlier. These files exist in S3 but are not referenced by Iceberg metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show orphaned files before cleanup\n",
        "print(\"=== Before Orphaned Files Cleanup ===\")\n",
        "print(\"Files in data directory (including orphaned):\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQL approach: CALL demo.system.remove_orphan_files(table => 'demo.default.pii_data', older_than => TIMESTAMP '2100-01-01 00:00:00')\n",
        "\n",
        "try:\n",
        "    # Try with a far-future date (this should FAIL due to safety protection)\n",
        "    print(\"Trying with far-future date (should FAIL due to safety protection):\")\n",
        "    result = spark.sql(\"\"\"\n",
        "        CALL demo.system.remove_orphan_files(\n",
        "            table => 'demo.default.pii_data',\n",
        "            older_than => TIMESTAMP '2100-01-01 00:00:00'\n",
        "        )\n",
        "    \"\"\")\n",
        "    result.show(truncate=False)\n",
        "    print(\"✅ SQL approach worked!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ SQL approach failed: {e}\")\n",
        "    print(\"This is EXPECTED! Iceberg has safety protections to prevent accidental deletion.\")\n",
        "    print(\"The safety window prevents deletion of files that might still be referenced.\")\n",
        "    print(\"Since we're in a controlled environment, we'll use the Action approach instead.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Action approach: cleanup_orphan_files(spark, 'demo.default.pii_data', method='action', cutoff='immediate')\n",
        "\n",
        "# Clean up orphaned files using the Action approach\n",
        "cleanup_orphan_files(spark, \"demo.default.pii_data\", method=\"action\", cutoff=\"immediate\")\n",
        "\n",
        "# Show files after cleanup\n",
        "print(\"\\n=== After Orphaned Files Cleanup ===\")\n",
        "print(\"Files in data directory (orphaned files removed):\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after orphaned files cleanup\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After Orphaned Files Cleanup\")\n",
        "print(\"\\nFile summary after orphaned files cleanup:\")\n",
        "all_current\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (Orphaned Files Cleanup) ===\")\n",
        "diff_cleanup = diff_summaries(all_previous, all_current)\n",
        "diff_cleanup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Rewrite Data Files (Apply Deletes / VACUUM)\n",
        "\n",
        "**MOR (Merge-on-Read) vs COW (Copy-on-Write) Deletion:**\n",
        "\n",
        "- **MOR**: Deleted rows still reside in data files until we **apply deletes**.\n",
        "  - `rewrite_data_files` applies delete files and rewrites Parquet files\n",
        "  - Deleted rows are physically purged from storage\n",
        "  - `expire_snapshots` drops old snapshots that reference pre-delete files\n",
        "\n",
        "- **COW**: Data files are rewritten immediately with the delete.\n",
        "  - Only `expire_snapshots` would be required (no rewrite needed)\n",
        "  - Deleted data is immediately removed from new data files\n",
        "\n",
        "Since we're using MOR, we need both operations for complete PII deletion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files before VACUUM\n",
        "print(\"=== Before VACUUM (Rewrite Data Files) ===\")\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"Before VACUUM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Step 3: Rewrite Data Files (apply deletes) ===\")\n",
        "\n",
        "# First, let's show that delete files still exist even after snapshot expiration and orphan cleanup\n",
        "print(\"\\n=== DANGER: Delete files still exist! ===\")\n",
        "print(\"Even after expiring snapshots and cleaning orphaned files, delete files remain:\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the delete files to show the PII still exists\n",
        "print(\"=== Examining Delete Files - PII Still Exists! ===\")\n",
        "examine_delete_files(spark)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Now Running VACUUM to Apply Deletes ===\")\n",
        "\n",
        "# Step 1: Rewrite data files to remove deleted rows\n",
        "print(\"Step 1: Rewriting data files...\")\n",
        "result1 = spark.sql(\"\"\"\n",
        "  CALL demo.system.rewrite_data_files(\n",
        "    table => 'default.pii_data',\n",
        "    options => map(\n",
        "      'rewrite-all','true',\n",
        "      'target-file-size-bytes','134217728'  -- 128 MiB example\n",
        "    )\n",
        "  )\n",
        "\"\"\")\n",
        "print(\"✅ rewrite_data_files finished (deleted rows removed from data files)\")\n",
        "result1.show(truncate=False)\n",
        "\n",
        "# Step 2: Rewrite position delete files\n",
        "print(\"\\nStep 2: Rewriting position delete files...\")\n",
        "result2 = spark.sql(\"\"\"\n",
        "  CALL demo.system.rewrite_position_delete_files(\n",
        "    table => 'default.pii_data',\n",
        "    options => map(\n",
        "      'rewrite-all','true',\n",
        "      'target-file-size-bytes','134217728'  -- 128 MiB example\n",
        "    )\n",
        "  )\n",
        "\"\"\")\n",
        "print(\"✅ rewrite_position_delete_files finished (delete files processed)\")\n",
        "result2.show(truncate=False)\n",
        "\n",
        "# Validate: delete files should be gone or reduced\n",
        "files_after = spark.table(\"demo.default.pii_data.files\")\n",
        "delete_file_count_after = files_after.filter(\"content IN (1,2)\").count()\n",
        "print(f\"Delete files AFTER rewrite_data_files: {delete_file_count_after}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== What rewrite_data_files Accomplished ===\")\n",
        "print(\"✅ Applied delete files (MOR): Deleted rows are now physically purged from Parquet files\")\n",
        "print(\"✅ Rewrote data files: Consolidated small files and removed deleted data\")\n",
        "print(\"✅ Physical deletion complete: PII data no longer exists in storage\")\n",
        "print(\"\\nNext: We need to expire snapshots to remove time travel access to pre-delete files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Validation: MOR deletes applied ===\")\n",
        "if delete_file_count_after < delete_file_count_before:\n",
        "    print(\"✅ Delete files reduced — MOR deletes were applied during rewrite.\")\n",
        "else:\n",
        "    print(\"⚠️ Delete files count did not drop. Check table props and engine versions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine delete files after VACUUM to show the difference\n",
        "print(\"=== Examining Delete Files After VACUUM ===\")\n",
        "examine_delete_files(spark)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files after VACUUM\n",
        "print(\"=== After VACUUM (Rewrite Data Files) ===\")\n",
        "all_previous = all_current.copy(deep=True)\n",
        "_, _, all_current = summarize_files(spark, table_base_path, \"After VACUUM\")\n",
        "\n",
        "# Show the difference\n",
        "print(\"\\n=== File Summary Comparison (After VACUUM) ===\")\n",
        "diff_vacuum = diff_summaries(all_previous, all_current)\n",
        "diff_vacuum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final snapshot expiration to complete the VACUUM cleanup\n",
        "print(\"=== Final Snapshot Expiration (Complete VACUUM Cleanup) ===\")\n",
        "print(\"Expiring any remaining old snapshots to complete the cleanup...\")\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
        "print(f\"Expiring snapshots older than: {now}\")\n",
        "\n",
        "spark.sql(f\"CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '{now}')\")\n",
        "print(\"✅ Final snapshot expiration completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final file state after complete cleanup\n",
        "print(\"=== Final File State After Complete Cleanup ===\")\n",
        "print(\"Files remaining in data directory after all operations:\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final table state\n",
        "print(\"\\n=== Final Table State ===\")\n",
        "print(\"Current table data (after all operations):\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Final Validation\n",
        "\n",
        "Let's verify that the PII has been permanently and irreversibly deleted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final file listing before validation\n",
        "print(\"=== Final File Listing ===\")\n",
        "print(\"Files remaining in data directory after all operations:\")\n",
        "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final verification\n",
        "print(\"=== Final Validation: PII Permanently Deleted ===\")\n",
        "\n",
        "# 1. Verify time travel is blocked\n",
        "print(\"=== Step 1: Verify Time Travel is Blocked ===\")\n",
        "# Get the latest snapshot ID (should be the only one remaining)\n",
        "latest_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
        "latest_snapshot_id = latest_snapshots.select(\"snapshot_id\").first()[0]\n",
        "print(f\"Latest snapshot ID: {latest_snapshot_id}\")\n",
        "\n",
        "try:\n",
        "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n",
        "except Exception as e:\n",
        "    print(\"✅ Time travel to old snapshots is blocked\")\n",
        "\n",
        "# 2. Verify current data (PII should be gone)\n",
        "print(\"\\n=== Step 2: Verify Current Data ===\")\n",
        "print(\"Current data (PII should be gone):\")\n",
        "df = spark.table(\"demo.default.pii_data\").toPandas()\n",
        "display(df)\n",
        "\n",
        "# 3. Verify table history (should only have one snapshot)\n",
        "print(\"\\n=== Step 3: Verify Table History ===\")\n",
        "print(\"Table history (should only have current snapshot):\")\n",
        "df_history = spark.table(\"demo.default.pii_data.history\").toPandas()\n",
        "display(df_history)\n",
        "\n",
        "# 4. Show final file state\n",
        "print(\"\\n=== Step 4: Final File State ===\")\n",
        "print(\"Final file state:\")\n",
        "_, _, all_final = summarize_files(spark, table_base_path, \"Final State\")\n",
        "all_final\n",
        "\n",
        "# 5. Final delete file examination\n",
        "print(\"\\n=== Step 5: Final Delete File Examination ===\")\n",
        "examine_delete_files(spark)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🎉 PII DELETION COMPLETE!\n",
        "\n",
        "# ✅ What we accomplished:\n",
        "- Logically deleted PII (set to NULL)\n",
        "- Expired old snapshots (blocked time travel)\n",
        "- Cleaned up orphaned files\n",
        "- Rewrote data files (physically removed PII)\n",
        "\n",
        "# ✅ Compliance achieved:\n",
        "- PII is permanently and irreversibly deleted\n",
        "- Time travel to old snapshots is impossible\n",
        "- No orphaned files remain\n",
        "- Data files no longer contain the PII\n",
        "\n",
        "This approach ensures compliance with GDPR, CCPA, and other data privacy regulations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Summary: What We Demonstrated & Key Takeaways\n",
        "\n",
        "## What We Actually Did\n",
        "This demo showed a complete PII deletion workflow using Apache Iceberg:\n",
        "\n",
        "1. **Created PII Data**: Inserted sample data with personally identifiable information\n",
        "2. **Demonstrated the Problem**: Showed that simple SQL `UPDATE` to NULL doesn't permanently remove PII\n",
        "3. **Simulated Previous Deletion**: Created merge-on-read delete files to show how PII persists after deletion\n",
        "4. **Exposed Data Persistence**: Revealed that PII remains accessible in delete files and through time travel\n",
        "5. **Applied Complete Data Erasure**: Used both data file rewriting AND position delete file processing\n",
        "6. **Expired Snapshots**: Removed old snapshots to block time travel access\n",
        "7. **Removed Orphaned Files**: Cleaned up unreferenced files that may contain PII\n",
        "8. **Verified Compliance**: Confirmed PII was permanently and irreversibly removed\n",
        "\n",
        "## Key Technical Insights\n",
        "\n",
        "**Merge-on-Read (MOR) Mode Challenges:**\n",
        "- Delete operations create separate delete files (position/equality deletes)\n",
        "- Original Parquet files still contain the PII data\n",
        "- Delete files can be read directly to expose the \"deleted\" PII\n",
        "- **Critical**: You MUST rewrite data files for RTE compliance in MOR mode\n",
        "\n",
        "**Complete Data Erasure Process:**\n",
        "- **Data file rewriting**: Physically removes deleted rows from Parquet files\n",
        "- **Position delete processing**: Handles and consolidates delete files\n",
        "- **Snapshot expiration**: Removes old snapshots to block time travel\n",
        "- **Orphan cleanup**: Removes unreferenced files\n",
        "\n",
        "## Critical Security Findings\n",
        "\n",
        "**Before Data Erasure:**\n",
        "- PII data remains in original Parquet files\n",
        "- Delete files contain references to the exact PII that was \"deleted\"\n",
        "- Time travel can restore the original PII data\n",
        "- Orphaned files may contain PII\n",
        "\n",
        "**After Complete Data Erasure:**\n",
        "- PII is physically removed from all data files\n",
        "- Delete files are processed and removed\n",
        "- Time travel to old snapshots is blocked\n",
        "- No orphaned files remain\n",
        "\n",
        "## Compliance & Operations\n",
        "\n",
        "**For GDPR/CCPA Compliance:**\n",
        "- **COW Mode**: Snapshot expiration + orphan cleanup is sufficient\n",
        "- **MOR Mode**: Data file rewriting + snapshot expiration + orphan cleanup is required\n",
        "- **Governance**: Maintain RTE ledger of snapshots that must never be restored\n",
        "- **Monitoring**: Track delete file counts and ensure they trend down after erasure\n",
        "\n",
        "**Production Considerations:**\n",
        "- Run data erasure operations during maintenance windows\n",
        "- Use dry-run options for orphan cleanup\n",
        "- Monitor S3 lifecycle rules to prevent external file deletion\n",
        "- Implement proper access controls and audit trails\n",
        "\n",
        "This demo proves that **logical deletion alone is insufficient** for PII compliance in Iceberg MOR mode - you must physically rewrite data files to achieve true data erasure.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}