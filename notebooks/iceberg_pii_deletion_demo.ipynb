{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg PII Data Deletion Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of creating an Iceberg table, adding data, deleting PII, and then permanently removing the history containing the PII.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import pyspark and set up our Spark session. The configuration for the S3 endpoint and Iceberg catalog is already handled by the `docker-compose.yml` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/03 11:16:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure Spark with S3A filesystem settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergPIIDemo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2,org.apache.hadoop:hadoop-common:3.3.2\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.catalog.demo.s3.endpoint = http://minio:9000\n",
      "spark.sql.catalogImplementation = in-memory\n",
      "spark.sql.catalog.demo.warehouse = s3://warehouse/wh/\n",
      "spark.sql.catalog.demo.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.demo.uri = http://rest:8181\n",
      "spark.sql.catalog.demo.type = rest\n",
      "spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog\n"
     ]
    }
   ],
   "source": [
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    if \"catalog\" in k:\n",
    "        print(k, \"=\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X DELETE http://rest:8181/v1/namespaces/default/tables/pii_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "def _format_exception_message(e: Exception) -> str:\n",
    "    msg = str(e)\n",
    "    m = re.search(r\"(NotFoundException|TABLE_OR_VIEW_NOT_FOUND|AnalysisException|ServiceFailureException)[^\\n]*\", msg)\n",
    "    return m.group(0) if m else (msg.splitlines()[0] if msg else \"Unknown error\")\n",
    "\n",
    "def summarize_files(input_param: str,\n",
    "                    operation_name: str,\n",
    "                    save_path: str = None,\n",
    "                    run_id: str = None):\n",
    "    \"\"\"\n",
    "    Build minute-bucketed file summaries for Iceberg metadata + data.\n",
    "\n",
    "    Returns: (meta_pd, data_pd, all_pd)  # now Pandas\n",
    "      - meta_df columns: prefix,file_type,file_format,created_minute,files_created,run_id,operation\n",
    "      - data_df columns: prefix,file_type,file_format,created_minute,files_created,run_id,operation\n",
    "      - all_df  = union(meta_df,data_df)\n",
    "\n",
    "    If save_path is provided, writes Parquet to:\n",
    "      {save_path}/summary/  partitioned by run_id\n",
    "    \"\"\"\n",
    "    print(f\"--- File Summary ({operation_name}) ---\")\n",
    "\n",
    "    table_name = input_param\n",
    "    if input_param.startswith(\"s3a://warehouse/default/pii_data\"):\n",
    "        table_name = \"demo.default.pii_data\"\n",
    "        print(\"Using table name for better reliability...\")\n",
    "\n",
    "    # Give this run a stable id if none provided\n",
    "    if run_id is None:\n",
    "        run_id = datetime.utcnow().isoformat(timespec=\"seconds\").replace(\":\", \"-\") + \"Z\"\n",
    "\n",
    "    meta_df = None\n",
    "    data_df = None\n",
    "\n",
    "    # --- Metadata files ---\n",
    "    try:\n",
    "        metadata_query = f\"\"\"\n",
    "        WITH manifest_lists AS (\n",
    "          SELECT manifest_list AS file_path,\n",
    "                 committed_at  AS created_at,\n",
    "                 'avro'        AS file_format,\n",
    "                 'snapshots'   AS file_type\n",
    "          FROM {table_name}.snapshots\n",
    "          WHERE manifest_list IS NOT NULL\n",
    "        ),\n",
    "        manifests AS (\n",
    "          SELECT m.path        AS file_path,\n",
    "                 s.committed_at AS created_at,\n",
    "                 'avro'         AS file_format,\n",
    "                 'manifests'    AS file_type\n",
    "          FROM {table_name}.all_manifests m\n",
    "          LEFT OUTER JOIN {table_name}.snapshots s\n",
    "            ON m.added_snapshot_id = s.snapshot_id\n",
    "          WHERE m.path IS NOT NULL\n",
    "        ),\n",
    "        metadata_json AS (\n",
    "          SELECT file         AS file_path,\n",
    "                 timestamp    AS created_at,\n",
    "                 'json'       AS file_format,\n",
    "                 'metadata_log_entries' AS file_type\n",
    "          FROM {table_name}.metadata_log_entries\n",
    "          WHERE file IS NOT NULL\n",
    "        )\n",
    "        SELECT\n",
    "          'metadata'                                 AS prefix,\n",
    "          file_type,\n",
    "          file_format,\n",
    "          date_trunc('minute', created_at)           AS created_minute,\n",
    "          COUNT(*)                                   AS files_created\n",
    "        FROM (\n",
    "          SELECT * FROM manifest_lists\n",
    "          UNION ALL\n",
    "          SELECT * FROM manifests\n",
    "          UNION ALL\n",
    "          SELECT * FROM metadata_json\n",
    "        )\n",
    "        GROUP BY prefix, file_type, file_format, date_trunc('minute', created_at)\n",
    "        \"\"\"\n",
    "        meta_df = spark.sql(metadata_query) \\\n",
    "                       .withColumn(\"run_id\", F.lit(run_id)) \\\n",
    "                       .withColumn(\"operation\", F.lit(operation_name))\n",
    "        print(\"\\nMetadata file summary:\")\n",
    "        meta_df.orderBy(\"created_minute\",\"file_type\",\"file_format\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Metadata file summary unavailable: {_format_exception_message(e)}\")\n",
    "\n",
    "    # --- Data files ---\n",
    "    try:\n",
    "        data_query = f\"\"\"\n",
    "        WITH created AS (\n",
    "          SELECT e.data_file.file_path AS file_path,\n",
    "                 MIN(s.committed_at)   AS created_at,\n",
    "                 MIN(e.data_file.content) AS content\n",
    "          FROM {table_name}.entries e\n",
    "          JOIN {table_name}.snapshots s USING (snapshot_id)\n",
    "          GROUP BY e.data_file.file_path\n",
    "        )\n",
    "        SELECT\n",
    "          'data' AS prefix,\n",
    "          CASE content\n",
    "            WHEN 0 THEN 'data'\n",
    "            WHEN 1 THEN 'position_deletes'\n",
    "            WHEN 2 THEN 'equality_deletes'\n",
    "            ELSE 'unknown'\n",
    "          END AS file_type,\n",
    "          /* file_format for data isn't tracked here; set to parquet for uniform schema */\n",
    "          'parquet' AS file_format,\n",
    "          date_trunc('minute', created_at) AS created_minute,\n",
    "          COUNT(*) AS files_created\n",
    "        FROM created\n",
    "        GROUP BY prefix, content, date_trunc('minute', created_at)\n",
    "        \"\"\"\n",
    "        data_df = spark.sql(data_query) \\\n",
    "                       .withColumn(\"run_id\", F.lit(run_id)) \\\n",
    "                       .withColumn(\"operation\", F.lit(operation_name))\n",
    "        print(\"\\nData file summary:\")\n",
    "        data_df.orderBy(\"created_minute\",\"file_type\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Data file summary unavailable: {_format_exception_message(e)}\")\n",
    "\n",
    "    # Union (align schemas if one side is None)\n",
    "    cols = [\"prefix\",\"file_type\",\"file_format\",\"created_minute\",\"files_created\",\"run_id\",\"operation\"]\n",
    "    def _empty_df():\n",
    "        return spark.createDataFrame([], \"prefix string, file_type string, file_format string, created_minute timestamp, files_created long, run_id string, operation string\")\n",
    "    if meta_df is None: meta_df = _empty_df()\n",
    "    if data_df is None: data_df = _empty_df()\n",
    "    all_df = meta_df.select(cols).unionByName(data_df.select(cols))\n",
    "\n",
    "    # Optional save\n",
    "    if save_path:\n",
    "        (all_df\n",
    "         .repartition(\"run_id\")\n",
    "         .write\n",
    "         .mode(\"append\")\n",
    "         .partitionBy(\"run_id\")\n",
    "         .parquet(f\"{save_path.rstrip('/')}/summary\"))\n",
    "        print(f\"\\nSaved summary to {save_path.rstrip('/')}/summary (partitioned by run_id={run_id})\")\n",
    "\n",
    "    # Return as Pandas (no extra prints)\n",
    "    return meta_df.toPandas(), data_df.toPandas(), all_df.toPandas()\n",
    "\n",
    "# Define the base path for our table for easy reuse \n",
    "table_base_path = \"s3a://warehouse/default/pii_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def diff_summaries(df_old: pd.DataFrame, df_new: pd.DataFrame, count_col: str = \"files_created\") -> pd.DataFrame:\n",
    "    def norm(df):\n",
    "        d = df.copy()\n",
    "        d[\"_prefix\"]      = d[\"prefix\"].astype(\"string\")\n",
    "        d[\"_file_type\"]   = d[\"file_type\"].astype(\"string\")\n",
    "        d[\"_file_format\"] = d.get(\"file_format\").astype(\"string\").fillna(\"__NULL__\")\n",
    "        d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n",
    "        return d\n",
    "\n",
    "    keys = [\"_prefix\",\"_file_type\",\"_file_format\",\"_minute_str\"]\n",
    "\n",
    "    oldN = norm(df_old)\n",
    "    newN = norm(df_new)\n",
    "\n",
    "    # aggregate + keep natural fields on each side\n",
    "    oldA = (oldN.groupby(keys, dropna=False)\n",
    "                 .agg(old_count=(count_col,\"sum\"),\n",
    "                      prefix=(\"prefix\",\"first\"),\n",
    "                      file_type=(\"file_type\",\"first\"),\n",
    "                      file_format=(\"file_format\",\"first\"),\n",
    "                      created_minute=(\"created_minute\",\"first\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    newA = (newN.groupby(keys, dropna=False)\n",
    "                 .agg(new_count=(count_col,\"sum\"),\n",
    "                      prefix=(\"prefix\",\"first\"),\n",
    "                      file_type=(\"file_type\",\"first\"),\n",
    "                      file_format=(\"file_format\",\"first\"),\n",
    "                      created_minute=(\"created_minute\",\"first\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    # present in both\n",
    "    both = newA.merge(oldA[keys + [\"old_count\"]], on=keys, how=\"inner\")\n",
    "    both = both.loc[:, [\"prefix\",\"file_type\",\"file_format\",\"created_minute\",\"old_count\",\"new_count\"]]\n",
    "    both[\"delta\"] = both[\"new_count\"] - both[\"old_count\"]\n",
    "    both[\"status\"] = np.where(both[\"delta\"].eq(0), \"UNCHANGED\", \"CHANGED\")\n",
    "\n",
    "    # added (in new only)\n",
    "    added = newA.merge(oldA[keys], on=keys, how=\"left\", indicator=True)\n",
    "    added = (added[added[\"_merge\"] == \"left_only\"]\n",
    "                  .drop(columns=\"_merge\")\n",
    "                  .assign(old_count=0)[[\"prefix\",\"file_type\",\"file_format\",\"created_minute\",\"old_count\",\"new_count\"]])\n",
    "    added[\"delta\"] = added[\"new_count\"]\n",
    "    added[\"status\"] = \"ADDED\"\n",
    "\n",
    "    # removed (in old only)\n",
    "    removed = oldA.merge(newA[keys], on=keys, how=\"left\", indicator=True)\n",
    "    removed = (removed[removed[\"_merge\"] == \"left_only\"]\n",
    "                    .drop(columns=\"_merge\")\n",
    "                    .assign(new_count=0)[[\"prefix\",\"file_type\",\"file_format\",\"created_minute\",\"old_count\",\"new_count\"]])\n",
    "    removed[\"delta\"] = -removed[\"old_count\"]\n",
    "    removed[\"status\"] = \"REMOVED\"\n",
    "\n",
    "    # union + prettify\n",
    "    diff = pd.concat([both, added, removed], ignore_index=True)\n",
    "    diff[\"minute_str\"] = pd.to_datetime(diff[\"created_minute\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return (diff[[\"prefix\",\"file_type\",\"file_format\",\"created_minute\",\"minute_str\",\n",
    "                  \"old_count\",\"new_count\",\"delta\",\"status\"]]\n",
    "            .sort_values([\"prefix\",\"file_type\",\"file_format\",\"created_minute\"])\n",
    "            .reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What each of these metadata file types “tells you”\n",
    "\n",
    "m*.avro (manifests): rows = data-file entries, with partition info + per-column stats + an entry status (ADDED/DELETED). They’re scoped to a snapshot (you can link them via added_snapshot_id).\n",
    "\n",
    "snap-*.avro (manifest lists): the index for a snapshot; each row points to one or more m*.avro files used by that snapshot.\n",
    "\n",
    "0000*-*.metadata.json (table metadata versions): the table’s high-level state over time and which snapshots are current/valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an Iceberg table called `pii_data` in our `demo` catalog. The schema will include the PII columns we want to manage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the REST catalog, we need to create a namespace before we can create a table. We'll create a namespace called `default` inside our `demo` catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.default.pii_data;\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.default\")\n",
    "df = spark.sql(\"SHOW TABLES IN demo.default;\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Table Creation) ---\n",
      "Using table name for better reliability...\n",
      "Metadata file summary unavailable: TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`default`.`pii_data`.`snapshots` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "Data file summary unavailable: TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`default`.`pii_data`.`entries` cannot be found. Verify the spelling and correctness of the schema and catalog.\n"
     ]
    }
   ],
   "source": [
    "# Check files after namespace creation\n",
    "_, _, all_previous = summarize_files(table_base_path, \"After Table Creation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.default.pii_data (\n",
    "    case_id STRING,\n",
    "    first_name STRING,\n",
    "    email_address STRING,\n",
    "    key_nm STRING,\n",
    "    secure_txt STRING,\n",
    "    secure_key STRING,\n",
    "    update_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Table Creation) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:16:00|1            |2025-09-03T11-16-27Z|After Table Creation|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "|prefix|file_type|file_format|created_minute|files_created|run_id|operation|\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-16-27Z</td>\n",
       "      <td>After Table Creation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "\n",
       "   files_created                run_id             operation  \n",
       "0              1  2025-09-03T11-16-27Z  After Table Creation  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after table creation\n",
    "_, _, all_previous = summarize_files(table_base_path, \"After Table Creation\")\n",
    "all_previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seed the Table with Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's insert some sample data into our table. We'll add two records, one of which we will target for PII deletion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO demo.default.pii_data VALUES\n",
    "('case-1', 'John', 'john.doe@example.com', 'key1', 'secret text 1', 'secret_key_1', DATE('2023-01-01')),\n",
    "('case-2', 'Jane', 'jane.doe@example.com', 'key2', 'secret text 2', 'secret_key_2', DATE('2023-01-02'))\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Data Insertion) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|metadata|manifests           |avro       |2025-09-03 11:16:00|1            |2025-09-03T11-16-29Z|After Data Insertion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:16:00|2            |2025-09-03T11-16-29Z|After Data Insertion|\n",
      "|metadata|snapshots           |avro       |2025-09-03 11:16:00|1            |2025-09-03T11-16-29Z|After Data Insertion|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|data  |data     |parquet    |2025-09-03 11:16:00|2            |2025-09-03T11-16-29Z|After Data Insertion|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-16-29Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-16-29Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-16-29Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-16-29Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "3      data                  data     parquet 2025-09-03 11:16:00   \n",
       "\n",
       "   files_created                run_id             operation  \n",
       "0              1  2025-09-03T11-16-29Z  After Data Insertion  \n",
       "1              1  2025-09-03T11-16-29Z  After Data Insertion  \n",
       "2              2  2025-09-03T11-16-29Z  After Data Insertion  \n",
       "3              2  2025-09-03T11-16-29Z  After Data Insertion  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after data insertion\n",
    "_, _, all_current = summarize_files(table_base_path, \"After Data Insertion\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n",
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-03 11:16:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "3  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-03 11:16:00          0          2      2    ADDED  \n",
       "1  2025-09-03 11:16:00          0          1      1    ADDED  \n",
       "2  2025-09-03 11:16:00          1          2      1  CHANGED  \n",
       "3  2025-09-03 11:16:00          0          1      1    ADDED  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the data is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      John|john.doe@example.com|  key1|secret text 1|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the table's history to see the snapshot that was created when we inserted the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-09-03 11:16:...|5408540653570735206|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
    "initial_snapshots.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete PII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will \"delete\" the PII for `case-1`. In this context, \"deletion\" means updating the PII columns to `NULL`. This is a common strategy for retaining the record for referential integrity while removing the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pii(case_id):\n",
    "    spark.sql(f\"\"\"\n",
    "    UPDATE demo.default.pii_data\n",
    "    SET\n",
    "        first_name = NULL,\n",
    "        email_address = NULL,\n",
    "        secure_txt = NULL\n",
    "    WHERE case_id = '{case_id}'\n",
    "    \"\"\")\n",
    "\n",
    "delete_pii('case-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Data Deletion) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation          |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|metadata|manifests           |avro       |2025-09-03 11:16:00|1            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:16:00|2            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|metadata|snapshots           |avro       |2025-09-03 11:16:00|1            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|metadata|manifests           |avro       |2025-09-03 11:17:00|2            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:17:00|1            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|metadata|snapshots           |avro       |2025-09-03 11:17:00|1            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation          |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|data  |data     |parquet    |2025-09-03 11:16:00|1            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "|data  |data     |parquet    |2025-09-03 11:17:00|2            |2025-09-03T11-17-11Z|After Data Deletion|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "1  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "2  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "3  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "5  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "6      data                  data     parquet 2025-09-03 11:17:00   \n",
       "7      data                  data     parquet 2025-09-03 11:16:00   \n",
       "\n",
       "   files_created                run_id            operation  \n",
       "0              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "1              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "2              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "3              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "4              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "5              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "6              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "7              1  2025-09-03T11-17-11Z  After Data Deletion  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after data deletion (updates)\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(table_base_path, \"After Data Deletion\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n",
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-03 11:16:00   \n",
       "1      data                  data     parquet 2025-09-03 11:17:00   \n",
       "2  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "3  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "5  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "6  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "7  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta     status  \n",
       "0  2025-09-03 11:16:00          2          1     -1    CHANGED  \n",
       "1  2025-09-03 11:17:00          0          2      2      ADDED  \n",
       "2  2025-09-03 11:16:00          1          1      0  UNCHANGED  \n",
       "3  2025-09-03 11:17:00          0          2      2      ADDED  \n",
       "4  2025-09-03 11:16:00          2          2      0  UNCHANGED  \n",
       "5  2025-09-03 11:17:00          0          1      1      ADDED  \n",
       "6  2025-09-03 11:16:00          1          1      0  UNCHANGED  \n",
       "7  2025-09-03 11:17:00          0          1      1      ADDED  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data again. We should see that the PII for `case-1` is now gone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the table history, we'll see a new snapshot has been added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-03 11:16:...|5408540653570735206|               NULL|               true|\n",
      "|2025-09-03 11:17:...|2222908239089185913|5408540653570735206|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data.history\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Problem: Time Travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've \"deleted\" the PII from the current view of the table, the old data still exists in the previous snapshot. Anyone with access can use time travel to see the PII.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      John|john.doe@example.com|  key1|secret text 1|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_snapshot_id = initial_snapshots.select(\"snapshot_id\").first()[0]\n",
    "spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Permanent Deletion with Maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To permanently remove the PII, we need to perform two maintenance operations:\n",
    "1.  **Expire Snapshots**: This removes old snapshots from the table's metadata, making time travel to those versions impossible.\n",
    "2.  **Rewrite Data Files (VACUUM)**: This physically rewrites the data files to remove data that is no longer referenced by any snapshot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expire Old Snapshots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll expire all snapshots that are older than the current one. We can get the current timestamp and use that to expire anything older.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-17-11Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "1  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "2  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "3  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "5  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "6      data                  data     parquet 2025-09-03 11:17:00   \n",
       "7      data                  data     parquet 2025-09-03 11:16:00   \n",
       "\n",
       "   files_created                run_id            operation  \n",
       "0              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "1              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "2              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "3              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "4              1  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "5              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "6              2  2025-09-03T11-17-11Z  After Data Deletion  \n",
       "7              1  2025-09-03T11-17-11Z  After Data Deletion  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-03 11:19:15.156012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "print(now)\n",
    "spark.sql(f\"CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '{now}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the history, we should only see the most recent snapshot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-03 11:17:...|2222908239089185913|5408540653570735206|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data.history\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Expire Snapshots) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation             |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:16:00|2            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "|metadata|manifests           |avro       |2025-09-03 11:17:00|2            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:17:00|1            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "|metadata|snapshots           |avro       |2025-09-03 11:17:00|1            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:19:00|1            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation             |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|data  |data     |parquet    |2025-09-03 11:17:00|2            |2025-09-03T11-19-34Z|After Expire Snapshots|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-03 11:19:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "5      data                  data     parquet 2025-09-03 11:17:00   \n",
       "\n",
       "   files_created                run_id               operation  \n",
       "0              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "1              2  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "2              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "3              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "4              2  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "5              2  2025-09-03T11-19-34Z  After Expire Snapshots  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(table_base_path, \"After Expire Snapshots\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n",
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-03 11:16:00   \n",
       "1      data                  data     parquet 2025-09-03 11:17:00   \n",
       "2  metadata             manifests        avro 2025-09-03 11:16:00   \n",
       "3  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "5  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "6  metadata  metadata_log_entries        json 2025-09-03 11:19:00   \n",
       "7  metadata             snapshots        avro 2025-09-03 11:16:00   \n",
       "8  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta     status  \n",
       "0  2025-09-03 11:16:00          1          0     -1    REMOVED  \n",
       "1  2025-09-03 11:17:00          2          2      0  UNCHANGED  \n",
       "2  2025-09-03 11:16:00          1          0     -1    REMOVED  \n",
       "3  2025-09-03 11:17:00          2          2      0  UNCHANGED  \n",
       "4  2025-09-03 11:16:00          2          2      0  UNCHANGED  \n",
       "5  2025-09-03 11:17:00          1          1      0  UNCHANGED  \n",
       "6  2025-09-03 11:19:00          0          1      1      ADDED  \n",
       "7  2025-09-03 11:16:00          1          0     -1    REMOVED  \n",
       "8  2025-09-03 11:17:00          1          1      0  UNCHANGED  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Data Files (VACUUM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the snapshots are gone, the underlying Parquet files containing the PII may still exist in S3. The `rewrite_data_files` procedure (similar to VACUUM in other systems) will consolidate data into new files and remove the old, unreferenced ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-19-34Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-03 11:19:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "5      data                  data     parquet 2025-09-03 11:17:00   \n",
       "\n",
       "   files_created                run_id               operation  \n",
       "0              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "1              2  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "2              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "3              1  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "4              2  2025-09-03T11-19-34Z  After Expire Snapshots  \n",
       "5              2  2025-09-03T11-19-34Z  After Expire Snapshots  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL demo.system.rewrite_data_files('default.pii_data')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Rewrite Data Files) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:16:00|2            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "|metadata|manifests           |avro       |2025-09-03 11:17:00|2            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:17:00|1            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "|metadata|snapshots           |avro       |2025-09-03 11:17:00|1            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-03 11:19:00|1            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|data  |data     |parquet    |2025-09-03 11:17:00|2            |2025-09-03T11-20-00Z|After Rewrite Data Files|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-03T11-20-00Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-03 11:19:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "5      data                  data     parquet 2025-09-03 11:17:00   \n",
       "\n",
       "   files_created                run_id                 operation  \n",
       "0              1  2025-09-03T11-20-00Z  After Rewrite Data Files  \n",
       "1              2  2025-09-03T11-20-00Z  After Rewrite Data Files  \n",
       "2              1  2025-09-03T11-20-00Z  After Rewrite Data Files  \n",
       "3              1  2025-09-03T11-20-00Z  After Rewrite Data Files  \n",
       "4              2  2025-09-03T11-20-00Z  After Rewrite Data Files  \n",
       "5              2  2025-09-03T11-20-00Z  After Rewrite Data Files  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(table_base_path, \"After Rewrite Data Files\")\n",
    "all_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n",
      "/tmp/ipykernel_21789/1332850807.py:10: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  d[\"_minute_str\"]  = pd.to_datetime(d[\"created_minute\"], errors=\"coerce\").dt.floor(\"T\").dt.strftime(\"%Y-%m-%d %H:%M:00\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2025-09-03 11:16:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>2025-09-03 11:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>2025-09-03 11:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-03 11:17:00   \n",
       "1  metadata             manifests        avro 2025-09-03 11:17:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-03 11:16:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-03 11:17:00   \n",
       "4  metadata  metadata_log_entries        json 2025-09-03 11:19:00   \n",
       "5  metadata             snapshots        avro 2025-09-03 11:17:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta     status  \n",
       "0  2025-09-03 11:17:00          2          2      0  UNCHANGED  \n",
       "1  2025-09-03 11:17:00          2          2      0  UNCHANGED  \n",
       "2  2025-09-03 11:16:00          2          2      0  UNCHANGED  \n",
       "3  2025-09-03 11:17:00          1          1      0  UNCHANGED  \n",
       "4  2025-09-03 11:19:00          1          1      0  UNCHANGED  \n",
       "5  2025-09-03 11:17:00          1          1      0  UNCHANGED  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to time travel back to the first snapshot. This should fail because the snapshot no longer exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully prevented time travel!\n",
      "Cannot find snapshot with ID 5408540653570735206\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n",
    "except Exception as e:\n",
    "    print(\"Successfully prevented time travel!\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Manual Parquet File Reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section adds a utility to upload a Parquet file from your local computer and display its contents. This is useful for inspecting individual data files, for example, if you download a file from the MinIO bucket to your machine and want to see what's inside to confirm that PII has been physically removed from the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (17.0.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.34.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for the uploader widget and Parquet reader\n",
    "%pip install ipywidgets pandas pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c04a5ca08b4cae962873be3aa1be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.parquet', description='Upload Parquet File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d104a381b6d4b5ab24b056ffeb23416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create a file upload widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.parquet',\n",
    "    description='Upload Parquet File',\n",
    "    multiple=False\n",
    ")\n",
    "\n",
    "# Create an output widget to display the DataFrame\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"\n",
    "    This function is triggered when a file is uploaded.\n",
    "    It reads the Parquet file and displays it as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not uploader.value:\n",
    "        return\n",
    "        \n",
    "    # Get the uploaded file info\n",
    "    uploaded_file = uploader.value[0]\n",
    "    file_content = uploaded_file['content']\n",
    "    \n",
    "    # Read the Parquet file content into a Pandas DataFrame\n",
    "    df = pd.read_parquet(io.BytesIO(file_content))\n",
    "    \n",
    "    # Display the DataFrame in the output widget\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Contents of {uploaded_file['name']}:\")\n",
    "        display(df)\n",
    "        \n",
    "    # Reset the uploader so the same file can be uploaded again if needed\n",
    "    uploader.value.clear()\n",
    "    uploader._counter = 0\n",
    "\n",
    "\n",
    "# Observe changes in the uploader's value\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "# Display the uploader and the output area\n",
    "display(uploader, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have successfully and permanently deleted the PII from our Iceberg table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(f\"\"\"SELECT * FROM demo.default.pii_data TIMESTAMP AS OF '2026-09-02 09:40:00'\"\"\");\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
