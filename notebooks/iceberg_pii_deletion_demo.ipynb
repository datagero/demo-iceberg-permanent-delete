{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg PII Data Deletion Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of creating an Iceberg table, adding data, deleting PII, and then permanently removing the history containing the PII.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import pyspark and set up our Spark session. The configuration for the S3 endpoint and Iceberg catalog is already handled by the `docker-compose.yml` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/iceberg/notebooks\n",
      "Utils directory: /home/iceberg/notebooks/utils\n",
      "Utils exists: True\n",
      "Successfully imported utilities from: /home/iceberg/notebooks/utils\n"
     ]
    }
   ],
   "source": [
    "# Import all utility functions using the import script\n",
    "exec(open('import_utils.py').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.5\n",
      "Vector()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JavaObject id=o160"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Wire up the s3 -> s3a mappings and MinIO creds on the JVM Hadoop conf\n",
    "print(\"Spark:\", spark.version)\n",
    "print(spark._jsc.sc().listJars())\n",
    "hconf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "hconf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hconf.set(\"fs.AbstractFileSystem.s3.impl\", \"org.apache.hadoop.fs.s3a.S3A\")\n",
    "\n",
    "hconf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "hconf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hconf.set(\"fs.s3a.access.key\", \"admin\")\n",
    "hconf.set(\"fs.s3a.secret.key\", \"password\")\n",
    "hconf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "hconf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "# 2) Sanity checks: these MUST NOT throw now\n",
    "spark._jvm.java.lang.Class.forName(\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "    spark._jvm.java.net.URI.create(\"s3://warehouse\"),\n",
    "    spark._jsc.hadoopConfiguration()\n",
    ")\n",
    "# spark._jvm.java.lang.Class.forName(\"org.apache.iceberg.actions.Actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.catalog.demo.s3.endpoint = http://minio:9000\n",
      "spark.sql.catalogImplementation = in-memory\n",
      "spark.sql.catalog.demo.warehouse = s3://warehouse/wh/\n",
      "spark.sql.catalog.demo.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.sql.catalog.demo.uri = http://rest:8181\n",
      "spark.sql.catalog.demo.type = rest\n",
      "spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog\n"
     ]
    }
   ],
   "source": [
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    if \"catalog\" in k:\n",
    "        print(k, \"=\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/iceberg/notebooks\n",
      "sys.path: ['/home/iceberg/notebooks/utils', '/opt/spark/python/lib/py4j-0.10.9.7-src.zip', '/tmp/spark-11fe5469-f066-4d05-ae79-0816cc0abd95/userFiles-0152f7ec-3077-490b-94f5-14d18ec2e5a1', '/opt/spark/python', '/home/iceberg/notebooks', '/usr/local/lib/python310.zip', '/usr/local/lib/python3.10', '/usr/local/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import datetime\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"sys.path:\", sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\":{\"message\":\"Table does not exist: default.pii_data\",\"type\":\"NoSuchTableException\",\"code\":404,\"stack\":[\"org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist: default.pii_data\",\"\\tat org.apache.iceberg.rest.CatalogHandlers.dropTable(CatalogHandlers.java:310)\",\"\\tat org.apache.iceberg.rest.RESTCatalogAdapter.handleRequest(RESTCatalogAdapter.java:405)\",\"\\tat org.apache.iceberg.rest.RESTServerCatalogAdapter.handleRequest(RESTServerCatalogAdapter.java:42)\",\"\\tat org.apache.iceberg.rest.RESTCatalogAdapter.execute(RESTCatalogAdapter.java:628)\",\"\\tat org.apache.iceberg.rest.RESTCatalogAdapter.execute(RESTCatalogAdapter.java:609)\",\"\\tat org.apache.iceberg.rest.RESTCatalogServlet.execute(RESTCatalogServlet.java:108)\",\"\\tat org.apache.iceberg.rest.RESTCatalogServlet.doDelete(RESTCatalogServlet.java:84)\",\"\\tat jakarta.servlet.http.HttpServlet.service(HttpServlet.java:526)\",\"\\tat jakarta.servlet.http.HttpServlet.service(HttpServlet.java:587)\",\"\\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:764)\",\"\\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:529)\",\"\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:131)\",\"\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:122)\",\"\\tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:790)\",\"\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:122)\",\"\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:223)\",\"\\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1381)\",\"\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:176)\",\"\\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:484)\",\"\\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:174)\",\"\\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1303)\",\"\\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:129)\",\"\\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:122)\",\"\\tat org.eclipse.jetty.server.Server.handle(Server.java:563)\",\"\\tat org.eclipse.jetty.server.HttpChannel$RequestDispatchable.dispatch(HttpChannel.java:1598)\",\"\\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:753)\",\"\\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:501)\",\"\\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:287)\",\"\\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:314)\",\"\\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:100)\",\"\\tat org.eclipse.jetty.io.SelectableChannelEndPoint$1.run(SelectableChannelEndPoint.java:53)\",\"\\tat org.eclipse.jetty.util.thread.strategy.AdaptiveExecutionStrategy.runTask(AdaptiveExecutionStrategy.java:421)\",\"\\tat org.eclipse.jetty.util.thread.strategy.AdaptiveExecutionStrategy.consumeTask(AdaptiveExecutionStrategy.java:390)\",\"\\tat org.eclipse.jetty.util.thread.strategy.AdaptiveExecutionStrategy.tryProduce(AdaptiveExecutionStrategy.java:277)\",\"\\tat org.eclipse.jetty.util.thread.strategy.AdaptiveExecutionStrategy.run(AdaptiveExecutionStrategy.java:199)\",\"\\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:411)\",\"\\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:969)\",\"\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.doRunJob(QueuedThreadPool.java:1194)\",\"\\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1149)\",\"\\tat java.base/java.lang.Thread.run(Thread.java:840)\"]}}"
     ]
    }
   ],
   "source": [
    "!curl -X DELETE http://rest:8181/v1/namespaces/default/tables/pii_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "import datetime\n",
    "\n",
    "# All utility functions are already imported from import_utils.py\n",
    "\n",
    "# Define the base path for our table for easy reuse \n",
    "table_base_path = \"s3a://warehouse/default/pii_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# All utility functions are already imported from import_utils.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What each of these metadata file types ‚Äútells you‚Äù\n",
    "\n",
    "m*.avro (manifests): rows = data-file entries, with partition info + per-column stats + an entry status (ADDED/DELETED). They‚Äôre scoped to a snapshot (you can link them via added_snapshot_id).\n",
    "\n",
    "snap-*.avro (manifest lists): the index for a snapshot; each row points to one or more m*.avro files used by that snapshot.\n",
    "\n",
    "0000*-*.metadata.json (table metadata versions): the table‚Äôs high-level state over time and which snapshots are current/valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an Iceberg table called `pii_data` in our `demo` catalog. The schema will include the PII columns we want to manage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the REST catalog, we need to create a namespace before we can create a table. We'll create a namespace called `default` inside our `demo` catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version: 3.3.4\n",
      "S3AFileSystem is present.\n"
     ]
    }
   ],
   "source": [
    "# What Hadoop version is Spark using?\n",
    "print(\"Hadoop version:\", spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n",
    "\n",
    "# Can JVM see the S3AFileSystem class?\n",
    "try:\n",
    "    spark._jvm.java.lang.Class.forName(\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    print(\"S3AFileSystem is present.\")\n",
    "except Exception as e:\n",
    "    print(\"S3AFileSystem NOT present:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.default.pii_data;\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.default\")\n",
    "df = spark.sql(\"SHOW TABLES IN demo.default;\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Table Creation) ---\n",
      "Using table name for better reliability...\n",
      "Metadata file summary unavailable: TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`default`.`pii_data`.`snapshots` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "Data file summary unavailable: TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`default`.`pii_data`.`entries` cannot be found. Verify the spelling and correctness of the schema and catalog.\n"
     ]
    }
   ],
   "source": [
    "# Check files after namespace creation\n",
    "_, _, all_previous = summarize_files(spark, table_base_path, \"After Table Creation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS demo.default.pii_data (\n",
    "    case_id STRING,\n",
    "    first_name STRING,\n",
    "    email_address STRING,\n",
    "    key_nm STRING,\n",
    "    secure_txt STRING,\n",
    "    secure_key STRING,\n",
    "    update_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Table Creation) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|1            |2025-09-10T18-36-40Z|After Table Creation|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "|prefix|file_type|file_format|created_minute|files_created|run_id|operation|\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "+------+---------+-----------+--------------+-------------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-36-40Z</td>\n",
       "      <td>After Table Creation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id             operation  \n",
       "0              1  2025-09-10T18-36-40Z  After Table Creation  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after table creation\n",
    "_, _, all_previous = summarize_files(spark, table_base_path, \"After Table Creation\")\n",
    "all_previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seed the Table with Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's insert some sample data into our table. We'll add two records, one of which we will target for PII deletion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO demo.default.pii_data VALUES\n",
    "('case-1', 'John', 'john.doe@example.com', 'key1', 'secret text 1', 'secret_key_1', DATE('2023-01-01')),\n",
    "('case-2', 'Jane', 'jane.doe@example.com', 'key2', 'secret text 2', 'secret_key_2', DATE('2023-01-02'))\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the data is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      John|john.doe@example.com|  key1|secret text 1|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-09-10 18:36:...|7545001130093648888|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
    "initial_snapshots.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Data Insertion) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-43Z|After Data Insertion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|2            |2025-09-10T18-36-43Z|After Data Insertion|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-43Z|After Data Insertion|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation           |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-43Z|After Data Insertion|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-36-43Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-36-43Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-36-43Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-36-43Z</td>\n",
       "      <td>After Data Insertion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id             operation  \n",
       "0              1  2025-09-10T18-36-43Z  After Data Insertion  \n",
       "1              1  2025-09-10T18-36-43Z  After Data Insertion  \n",
       "2              2  2025-09-10T18-36-43Z  After Data Insertion  \n",
       "3              2  2025-09-10T18-36-43Z  After Data Insertion  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after data insertion\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Data Insertion\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-10 18:36:00          0          2      2    ADDED  \n",
       "1  2025-09-10 18:36:00          0          1      1    ADDED  \n",
       "2  2025-09-10 18:36:00          1          2      1  CHANGED  \n",
       "3  2025-09-10 18:36:00          0          1      1    ADDED  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Create Orphaned Files (After Data Insertion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some orphaned files to demonstrate cleanup later. These files will exist in S3 but won't be tracked by Iceberg metadata, simulating a failed write operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the data is there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files under data/ recursively:\n",
      "2025-09-10 18:36:43  s3://warehouse/default/pii_data/data/00000-22-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet\n",
      "2025-09-10 18:36:43  s3://warehouse/default/pii_data/data/00001-23-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "# üîç List files + modification dates\n",
    "print(\"All files under data/ recursively:\")\n",
    "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"x\",)], [\"dummy\"])\n",
    "df.write.mode(\"overwrite\").parquet(\"s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files under data/ recursively:\n",
      "2025-09-10 18:36:43  s3://warehouse/default/pii_data/data/00000-22-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet\n",
      "2025-09-10 18:36:43  s3://warehouse/default/pii_data/data/00001-23-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/_SUCCESS\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/part-00000-423ce4a8-d427-4e58-aa04-155fd5bf4d5c-c000.snappy.parquet\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/part-00001-423ce4a8-d427-4e58-aa04-155fd5bf4d5c-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# üîç List files + modification dates\n",
    "print(\"All files under data/ recursively:\")\n",
    "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      John|john.doe@example.com|  key1|secret text 1|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the table's history to see the snapshot that was created when we inserted the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete PII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will \"delete\" the PII for `case-1`. In this context, \"deletion\" means updating the PII columns to `NULL`. This is a common strategy for retaining the record for referential integrity while removing the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All utility functions are already imported from import_utils.py\n",
    "\n",
    "delete_pii(spark, 'case-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After PII Deletion ===\n",
      "--- File Summary (After PII Deletion) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation         |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|3            |2025-09-10T18-36-45Z|After PII Deletion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|3            |2025-09-10T18-36-45Z|After PII Deletion|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-45Z|After PII Deletion|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation         |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-45Z|After PII Deletion|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "=== File Summary Comparison (After PII Deletion) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-10 18:36:00          2          3      1  CHANGED  \n",
       "1  2025-09-10 18:36:00          1          3      2  CHANGED  \n",
       "2  2025-09-10 18:36:00          2          3      1  CHANGED  \n",
       "3  2025-09-10 18:36:00          1          2      1  CHANGED  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after PII deletion\n",
    "print(\"=== After PII Deletion ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After PII Deletion\")\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n=== File Summary Comparison (After PII Deletion) ===\")\n",
    "diff_pii = diff_summaries(all_previous, all_current)\n",
    "diff_pii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Problem: Time Travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've \"deleted\" the PII from the current view of the table, the old data still exists in the previous snapshot. Anyone with access can use time travel to see the PII.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-10 18:36:...|7545001130093648888|               NULL|               true|\n",
      "|2025-09-10 18:36:...|5013975016408165402|7545001130093648888|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Time traveling back to snapshot 7545001130093648888 to see the PII:\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      John|john.doe@example.com|  key1|secret text 1|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's verify the data is there\n",
    "spark.table(\"demo.default.pii_data\").show()\n",
    "\n",
    "# We can also inspect the table's history to see the snapshots\n",
    "initial_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
    "initial_snapshots.show()\n",
    "\n",
    "# Time travel back to the first snapshot to see the PII\n",
    "first_snapshot_id = initial_snapshots.select(\"snapshot_id\").first()[0]\n",
    "print(f\"\\nTime traveling back to snapshot {first_snapshot_id} to see the PII:\")\n",
    "spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expire Snapshots (Time Travel Cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To permanently remove the PII, we need to expire old snapshots. This removes old snapshots from the table's metadata, making time travel to those versions impossible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Expiring Snapshots ===\n",
      "--- File Summary (Before Expiring Snapshots) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|3            |2025-09-10T18-36-47Z|Before Expiring Snapshots|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|3            |2025-09-10T18-36-47Z|Before Expiring Snapshots|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-47Z|Before Expiring Snapshots|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-47Z|Before Expiring Snapshots|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------------+\n",
      "\n",
      "Expiring snapshots older than: 2025-09-10 18:36:47.738460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Expiring Snapshots ===\n",
      "--- File Summary (After Expiring Snapshots) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-49Z|After Expiring Snapshots|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|4            |2025-09-10T18-36-49Z|After Expiring Snapshots|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-49Z|After Expiring Snapshots|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-49Z|After Expiring Snapshots|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "=== File Summary Comparison (After Expiring Snapshots) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-10 18:36:00          3          2     -1  CHANGED  \n",
       "1  2025-09-10 18:36:00          3          2     -1  CHANGED  \n",
       "2  2025-09-10 18:36:00          3          4      1  CHANGED  \n",
       "3  2025-09-10 18:36:00          2          1     -1  CHANGED  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files before expiring snapshots\n",
    "print(\"=== Before Expiring Snapshots ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"Before Expiring Snapshots\")\n",
    "\n",
    "# Expire all snapshots older than current timestamp\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "print(f\"Expiring snapshots older than: {now}\")\n",
    "\n",
    "spark.sql(f\"CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '{now}')\")\n",
    "\n",
    "# Check files after expiring snapshots\n",
    "print(\"\\n=== After Expiring Snapshots ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Expiring Snapshots\")\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n=== File Summary Comparison (After Expiring Snapshots) ===\")\n",
    "diff_expire = diff_summaries(all_previous, all_current)\n",
    "diff_expire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Orphaned Files Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate that the orphaned files we created earlier still exist, and then clean them up. These files exist in S3 but are not referenced by Iceberg metadata, so they can be safely removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files under data/ recursively:\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/00000-58-5b319bd6-f4a9-425f-a060-16e95f36f6da-0-00001.parquet\n",
      "2025-09-10 18:36:43  s3://warehouse/default/pii_data/data/00001-23-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/_SUCCESS\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/part-00000-423ce4a8-d427-4e58-aa04-155fd5bf4d5c-c000.snappy.parquet\n",
      "2025-09-10 18:36:45  s3://warehouse/default/pii_data/data/ZZZ_orphan_test.parquet/part-00001-423ce4a8-d427-4e58-aa04-155fd5bf4d5c-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# üîç List files + modification dates\n",
    "print(\"All files under data/ recursively:\")\n",
    "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# All utility functions are already imported from import_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Iceberg DeleteOrphanFiles Action ‚Ä¶\n",
      "‚úì Orphaned files cleanup (Action) completed for demo.default.pii_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JavaObject id=o739"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_orphan_files(spark, \"demo.default.pii_data\", method=\"action\", cutoff=\"immediate\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîç List files + modification dates\n",
    "print(\"All files under data/ recursively:\")\n",
    "ls_s3_recursive(spark, \"s3://warehouse/default/pii_data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|file_path                                                                                         |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|s3://warehouse/default/pii_data/data/00000-58-5b319bd6-f4a9-425f-a060-16e95f36f6da-0-00001.parquet|\n",
      "|s3://warehouse/default/pii_data/data/00001-23-13d4bdd3-e848-41fd-847d-7a767e44108c-0-00001.parquet|\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT file_path FROM demo.default.pii_data.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Orphaned Files Cleanup ===\n",
      "--- File Summary (After Orphaned Files Cleanup) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-51Z|After Orphaned Files Cleanup|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|4            |2025-09-10T18-36-51Z|After Orphaned Files Cleanup|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-51Z|After Orphaned Files Cleanup|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-51Z|After Orphaned Files Cleanup|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "=== File Summary Comparison (After Orphaned Files Cleanup) ===\n",
      "\n",
      "üéØ Key Points about remove_orphan_files:\n",
      "   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\n",
      "   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\n",
      "   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\n",
      "   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup worked\n",
    "print(\"\\n=== After Orphaned Files Cleanup ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Orphaned Files Cleanup\")\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n=== File Summary Comparison (After Orphaned Files Cleanup) ===\")\n",
    "diff_cleanup = diff_summaries(all_previous, all_current)\n",
    "diff_cleanup\n",
    "\n",
    "print(\"\\nüéØ Key Points about remove_orphan_files:\")\n",
    "print(\"   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\")\n",
    "print(\"   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\")\n",
    "print(\"   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\")\n",
    "print(\"   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. VACUUM Operation (Rewrite Data Files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the snapshots are gone, the underlying Parquet files containing the PII may still exist in S3. The `rewrite_data_files` procedure (similar to VACUUM in other systems) will consolidate data into new files and remove the old, unreferenced ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before VACUUM (Rewrite Data Files) ===\n",
      "--- File Summary (Before VACUUM) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation    |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-52Z|Before VACUUM|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|4            |2025-09-10T18-36-52Z|Before VACUUM|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-52Z|Before VACUUM|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation    |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-52Z|Before VACUUM|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------+\n",
      "\n",
      "\n",
      "=== Running VACUUM (Rewrite Data Files) ===\n",
      "‚úì VACUUM completed\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         0|                     0|                    0|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "\n",
      "=== After VACUUM (Rewrite Data Files) ===\n",
      "--- File Summary (After VACUUM) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation   |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-52Z|After VACUUM|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|4            |2025-09-10T18-36-52Z|After VACUUM|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-52Z|After VACUUM|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation   |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-52Z|After VACUUM|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------+\n",
      "\n",
      "\n",
      "=== File Summary Comparison (After VACUUM) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta     status  \n",
       "0  2025-09-10 18:36:00          2          2      0  UNCHANGED  \n",
       "1  2025-09-10 18:36:00          2          2      0  UNCHANGED  \n",
       "2  2025-09-10 18:36:00          4          4      0  UNCHANGED  \n",
       "3  2025-09-10 18:36:00          1          1      0  UNCHANGED  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files before VACUUM\n",
    "print(\"=== Before VACUUM (Rewrite Data Files) ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"Before VACUUM\")\n",
    "\n",
    "# Run VACUUM (rewrite_data_files)\n",
    "print(\"\\n=== Running VACUUM (Rewrite Data Files) ===\")\n",
    "result = spark.sql(\"CALL demo.system.rewrite_data_files('default.pii_data')\")\n",
    "print(\"‚úì VACUUM completed\")\n",
    "result.show()\n",
    "\n",
    "# Check files after VACUUM\n",
    "print(\"\\n=== After VACUUM (Rewrite Data Files) ===\")\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After VACUUM\")\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n=== File Summary Comparison (After VACUUM) ===\")\n",
    "diff_vacuum = diff_summaries(all_previous, all_current)\n",
    "diff_vacuum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to time travel back to the first snapshot. This should fail because the snapshot no longer exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully prevented time travel!\n",
      "Error: Cannot find snapshot with ID 7545001130093648888\n",
      "\n",
      "=== Current Data (PII Should Be Gone) ===\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Table History (Should Only Have One Snapshot) ===\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-10 18:36:...|5013975016408165402|7545001130093648888|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "üéâ PII has been permanently deleted!\n",
      "   ‚Ä¢ Time travel to old snapshots is no longer possible\n",
      "   ‚Ä¢ Orphaned files have been cleaned up\n",
      "   ‚Ä¢ Data files have been rewritten to remove unreferenced data\n"
     ]
    }
   ],
   "source": [
    "# Try to time travel back to the first snapshot - this should fail\n",
    "try:\n",
    "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n",
    "except Exception as e:\n",
    "    print(\"‚úÖ Successfully prevented time travel!\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Verify the current data (PII should be gone)\n",
    "print(\"\\n=== Current Data (PII Should Be Gone) ===\")\n",
    "spark.table(\"demo.default.pii_data\").show()\n",
    "\n",
    "# Check the table history (should only have one snapshot)\n",
    "print(\"\\n=== Table History (Should Only Have One Snapshot) ===\")\n",
    "spark.table(\"demo.default.pii_data.history\").show()\n",
    "\n",
    "print(\"\\nüéâ PII has been permanently deleted!\")\n",
    "print(\"   ‚Ä¢ Time travel to old snapshots is no longer possible\")\n",
    "print(\"   ‚Ä¢ Orphaned files have been cleaned up\")\n",
    "print(\"   ‚Ä¢ Data files have been rewritten to remove unreferenced data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Up Orphaned Files ===\n",
      "Running remove_orphan_files to scan storage and remove unreferenced files...\n",
      "‚úì Orphaned files cleanup completed\n",
      "+--------------------+\n",
      "|orphan_file_location|\n",
      "+--------------------+\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's use remove_orphan_files to clean up the orphaned files\n",
    "print(\"\\n=== Cleaning Up Orphaned Files ===\")\n",
    "print(\"Running remove_orphan_files to scan storage and remove unreferenced files...\")\n",
    "\n",
    "try:\n",
    "    # Remove orphaned files - this scans the storage and compares against table metadata\n",
    "    result = spark.sql(\"CALL demo.system.remove_orphan_files('default.pii_data')\")\n",
    "    print(\"‚úì Orphaned files cleanup completed\")\n",
    "    result.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error during orphaned files cleanup: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Orphaned Files Cleanup ===\n",
      "--- File Summary (After Orphaned Files Cleanup) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-53Z|After Orphaned Files Cleanup|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|4            |2025-09-10T18-36-53Z|After Orphaned Files Cleanup|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-36-53Z|After Orphaned Files Cleanup|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-36-53Z|After Orphaned Files Cleanup|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "=== File Summary Comparison (After Cleanup) ===\n",
      "\n",
      "üéØ Key Points about remove_orphan_files:\n",
      "   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\n",
      "   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\n",
      "   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\n",
      "   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup worked\n",
    "print(\"\\n=== After Orphaned Files Cleanup ===\")\n",
    "_, _, all_after_cleanup = summarize_files(spark, table_base_path, \"After Orphaned Files Cleanup\")\n",
    "\n",
    "# Show the difference\n",
    "print(\"\\n=== File Summary Comparison (After Cleanup) ===\")\n",
    "diff_cleanup = diff_summaries(all_current, all_after_cleanup)\n",
    "diff_cleanup\n",
    "\n",
    "print(\"\\nüéØ Key Points about remove_orphan_files:\")\n",
    "print(\"   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\")\n",
    "print(\"   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\")\n",
    "print(\"   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\")\n",
    "print(\"   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-10 18:36:...|5013975016408165402|7545001130093648888|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_snapshots = spark.table(\"demo.default.pii_data.history\")\n",
    "initial_snapshots.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete PII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will \"delete\" the PII for `case-1`. In this context, \"deletion\" means updating the PII columns to `NULL`. This is a common strategy for retaining the record for referential integrity while removing the sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pii(case_id):\n",
    "    spark.sql(f\"\"\"\n",
    "    UPDATE demo.default.pii_data\n",
    "    SET\n",
    "        first_name = NULL,\n",
    "        email_address = NULL,\n",
    "        secure_txt = NULL\n",
    "    WHERE case_id = '{case_id}'\n",
    "    \"\"\")\n",
    "\n",
    "delete_pii('case-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Data Deletion) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation          |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-54Z|After Data Deletion|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-54Z|After Data Deletion|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-54Z|After Data Deletion|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation          |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-54Z|After Data Deletion|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-36-54Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-10T18-36-54Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-10T18-36-54Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-10T18-36-54Z</td>\n",
       "      <td>After Data Deletion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id            operation  \n",
       "0              2  2025-09-10T18-36-54Z  After Data Deletion  \n",
       "1              5  2025-09-10T18-36-54Z  After Data Deletion  \n",
       "2              5  2025-09-10T18-36-54Z  After Data Deletion  \n",
       "3              3  2025-09-10T18-36-54Z  After Data Deletion  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files after data deletion (updates)\n",
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Data Deletion\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Orphaned Files Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes files can become \"orphaned\" - they exist in storage but are not referenced by Iceberg metadata. This can happen due to:\n",
    "\n",
    "- **Partial/failed writes** that left files behind\n",
    "- **Manual file copies** to the wrong location\n",
    "- **Failed operations** that created files but didn't update metadata\n",
    "- **Direct S3 operations** that bypassed Iceberg\n",
    "\n",
    "In our demo, we already created orphaned files during the initial data load (see the `simulate_failed_write()` function above). Let's now demonstrate how to clean them up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Current State (Including Orphaned Files) ===\n",
      "Note: Orphaned files don't appear in the Iceberg metadata summary\n",
      "because they were written directly to S3, bypassing Iceberg's metadata tracking.\n",
      "--- File Summary (Current State with Orphaned Files) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                        |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-55Z|Current State with Orphaned Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-55Z|Current State with Orphaned Files|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-55Z|Current State with Orphaned Files|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                        |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-55Z|Current State with Orphaned Files|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the current state - note that orphaned files don't appear in Iceberg metadata\n",
    "print(\"=== Current State (Including Orphaned Files) ===\")\n",
    "print(\"Note: Orphaned files don't appear in the Iceberg metadata summary\")\n",
    "print(\"because they were written directly to S3, bypassing Iceberg's metadata tracking.\")\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"Current State with Orphaned Files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Up Orphaned Files ===\n",
      "Running remove_orphan_files to scan storage and remove unreferenced files...\n",
      "‚úì Orphaned files cleanup completed\n",
      "+--------------------+\n",
      "|orphan_file_location|\n",
      "+--------------------+\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's use remove_orphan_files to clean up the orphaned files\n",
    "print(\"\\n=== Cleaning Up Orphaned Files ===\")\n",
    "print(\"Running remove_orphan_files to scan storage and remove unreferenced files...\")\n",
    "\n",
    "try:\n",
    "    # Remove orphaned files - this scans the storage and compares against table metadata\n",
    "    result = spark.sql(\"CALL demo.system.remove_orphan_files('default.pii_data')\")\n",
    "    print(\"‚úì Orphaned files cleanup completed\")\n",
    "    result.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error during orphaned files cleanup: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Orphaned Files Cleanup ===\n",
      "--- File Summary (After Orphaned Files Cleanup) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-56Z|After Orphaned Files Cleanup|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-56Z|After Orphaned Files Cleanup|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-56Z|After Orphaned Files Cleanup|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-56Z|After Orphaned Files Cleanup|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "üéØ Key Points about remove_orphan_files:\n",
      "   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\n",
      "   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\n",
      "   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\n",
      "   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup worked\n",
    "print(\"\\n=== After Orphaned Files Cleanup ===\")\n",
    "_, _, all_after_cleanup = summarize_files(spark, table_base_path, \"After Orphaned Files Cleanup\")\n",
    "\n",
    "print(\"\\nüéØ Key Points about remove_orphan_files:\")\n",
    "print(\"   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\")\n",
    "print(\"   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\")\n",
    "print(\"   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\")\n",
    "print(\"   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-10 18:36:00          2          3      1  CHANGED  \n",
       "1  2025-09-10 18:36:00          2          5      3  CHANGED  \n",
       "2  2025-09-10 18:36:00          4          5      1  CHANGED  \n",
       "3  2025-09-10 18:36:00          1          2      1  CHANGED  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Orphaned Files Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes files can become \"orphaned\" - they exist in storage but are not referenced by Iceberg metadata. This can happen due to:\n",
    "\n",
    "- **Partial/failed writes** that left files behind\n",
    "- **Manual file copies** to the wrong location\n",
    "- **Failed operations** that created files but didn't update metadata\n",
    "- **Direct S3 operations** that bypassed Iceberg\n",
    "\n",
    "Let's demonstrate this by creating some orphaned files and then cleaning them up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Creating Orphaned Files ===\n",
      "--- File Summary (Before Creating Orphaned Files) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                     |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-56Z|Before Creating Orphaned Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-56Z|Before Creating Orphaned Files|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-56Z|Before Creating Orphaned Files|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                     |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-56Z|Before Creating Orphaned Files|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's check the current state before creating orphaned files\n",
    "print(\"=== Before Creating Orphaned Files ===\")\n",
    "_, _, all_before_orphans = summarize_files(spark, table_base_path, \"Before Creating Orphaned Files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating orphaned files: There is a 'path' option set and save() is called with a path parameter. Either remove the path option, or call save() without the parameter. To ignore this check, set 'spark.sql.legacy.pathOptionBehavior.enabled' to 'true'.\n"
     ]
    }
   ],
   "source": [
    "# Create some orphaned files by writing directly to S3 (bypassing Iceberg)\n",
    "# This simulates what might happen with failed writes or manual operations\n",
    "\n",
    "create_orphaned_files(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Creating Orphaned Files ===\n",
      "--- File Summary (After Creating Orphaned Files) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                    |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-57Z|After Creating Orphaned Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-57Z|After Creating Orphaned Files|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-57Z|After Creating Orphaned Files|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                    |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-57Z|After Creating Orphaned Files|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+-----------------------------+\n",
      "\n",
      "\n",
      "üìù Note: The orphaned files don't appear in the Iceberg metadata summary\n",
      "   because they were written directly to S3, bypassing Iceberg's metadata tracking.\n"
     ]
    }
   ],
   "source": [
    "# Check the state after creating orphaned files\n",
    "# Note: The orphaned files won't show up in our Iceberg metadata summary\n",
    "# because they're not tracked by Iceberg\n",
    "print(\"\\n=== After Creating Orphaned Files ===\")\n",
    "_, _, all_after_orphans = summarize_files(spark, table_base_path, \"After Creating Orphaned Files\")\n",
    "\n",
    "print(\"\\nüìù Note: The orphaned files don't appear in the Iceberg metadata summary\")\n",
    "print(\"   because they were written directly to S3, bypassing Iceberg's metadata tracking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Up Orphaned Files ===\n",
      "Running remove_orphan_files to scan storage and remove unreferenced files...\n",
      "‚úì Orphaned files cleanup completed\n",
      "+--------------------+\n",
      "|orphan_file_location|\n",
      "+--------------------+\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's use remove_orphan_files to clean up the orphaned files\n",
    "print(\"\\n=== Cleaning Up Orphaned Files ===\")\n",
    "print(\"Running remove_orphan_files to scan storage and remove unreferenced files...\")\n",
    "\n",
    "try:\n",
    "    # Remove orphaned files - this scans the storage and compares against table metadata\n",
    "    result = spark.sql(\"CALL demo.system.remove_orphan_files('default.pii_data')\")\n",
    "    print(\"‚úì Orphaned files cleanup completed\")\n",
    "    result.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error during orphaned files cleanup: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After Orphaned Files Cleanup ===\n",
      "--- File Summary (After Orphaned Files Cleanup) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|5            |2025-09-10T18-36-58Z|After Orphaned Files Cleanup|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|5            |2025-09-10T18-36-58Z|After Orphaned Files Cleanup|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-36-58Z|After Orphaned Files Cleanup|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation                   |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|3            |2025-09-10T18-36-58Z|After Orphaned Files Cleanup|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------------+\n",
      "\n",
      "\n",
      "üéØ Key Points about remove_orphan_files:\n",
      "   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\n",
      "   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\n",
      "   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\n",
      "   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleanup worked\n",
    "print(\"\\n=== After Orphaned Files Cleanup ===\")\n",
    "_, _, all_after_cleanup = summarize_files(spark, table_base_path, \"After Orphaned Files Cleanup\")\n",
    "\n",
    "print(\"\\nüéØ Key Points about remove_orphan_files:\")\n",
    "print(\"   ‚Ä¢ Scans the actual storage (S3) and compares against Iceberg metadata\")\n",
    "print(\"   ‚Ä¢ Removes files that exist in storage but are NOT referenced by any snapshot\")\n",
    "print(\"   ‚Ä¢ Different from expire_snapshots (which removes referenced but old files)\")\n",
    "print(\"   ‚Ä¢ Essential for cleaning up failed writes, manual copies, or direct S3 operations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to time travel back to the first snapshot. This should fail because the snapshot no longer exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manual Parquet File Reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data again. We should see that the PII for `case-1` is now gone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the table history, we'll see a new snapshot has been added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-10 18:36:...|5013975016408165402|7545001130093648888|               true|\n",
      "|2025-09-10 18:36:...|3743604877359090994|5013975016408165402|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data.history\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Problem: Time Travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've \"deleted\" the PII from the current view of the table, the old data still exists in the previous snapshot. Anyone with access can use time travel to see the PII.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_snapshot_id = initial_snapshots.select(\"snapshot_id\").first()[0]\n",
    "spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Permanent Deletion with Maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To permanently remove the PII, we need to perform two maintenance operations:\n",
    "1.  **Expire Snapshots**: This removes old snapshots from the table's metadata, making time travel to those versions impossible.\n",
    "2.  **Rewrite Data Files (VACUUM)**: This physically rewrites the data files to remove data that is no longer referenced by any snapshot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expire Old Snapshots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll expire all snapshots that are older than the current one. We can get the current timestamp and use that to expire anything older.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-36-55Z</td>\n",
       "      <td>Current State with Orphaned Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-10T18-36-55Z</td>\n",
       "      <td>Current State with Orphaned Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-10T18-36-55Z</td>\n",
       "      <td>Current State with Orphaned Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-10T18-36-55Z</td>\n",
       "      <td>Current State with Orphaned Files</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "3      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id                          operation  \n",
       "0              2  2025-09-10T18-36-55Z  Current State with Orphaned Files  \n",
       "1              5  2025-09-10T18-36-55Z  Current State with Orphaned Files  \n",
       "2              5  2025-09-10T18-36-55Z  Current State with Orphaned Files  \n",
       "3              3  2025-09-10T18-36-55Z  Current State with Orphaned Files  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-10 18:36:58.916577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "now = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "print(now)\n",
    "spark.sql(f\"CALL demo.system.expire_snapshots('default.pii_data', TIMESTAMP '{now}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the history, we should only see the most recent snapshot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-09-10 18:36:...|3743604877359090994|5013975016408165402|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"demo.default.pii_data.history\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Expire Snapshots) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation             |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|metadata|manifests           |avro       |NULL               |1            |2025-09-10T18-37-00Z|After Expire Snapshots|\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-37-00Z|After Expire Snapshots|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|6            |2025-09-10T18-37-00Z|After Expire Snapshots|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-37-00Z|After Expire Snapshots|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation             |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-37-00Z|After Expire Snapshots|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro                 NaT   \n",
       "2  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "4      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id               operation  \n",
       "0              1  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "1              1  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "2              2  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "3              6  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "4              2  2025-09-10T18-37-00Z  After Expire Snapshots  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Expire Snapshots\")\n",
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>CHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata             manifests        avro                 NaT   \n",
       "3  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "4  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta   status  \n",
       "0  2025-09-10 18:36:00          3          2     -1  CHANGED  \n",
       "1  2025-09-10 18:36:00          5          2     -3  CHANGED  \n",
       "2                  NaN          0          1      1    ADDED  \n",
       "3  2025-09-10 18:36:00          5          6      1  CHANGED  \n",
       "4  2025-09-10 18:36:00          2          1     -1  CHANGED  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Data Files (VACUUM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the snapshots are gone, the underlying Parquet files containing the PII may still exist in S3. The `rewrite_data_files` procedure (similar to VACUUM in other systems) will consolidate data into new files and remove the old, unreferenced ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-00Z</td>\n",
       "      <td>After Expire Snapshots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro                 NaT   \n",
       "2  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "4      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id               operation  \n",
       "0              1  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "1              1  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "2              2  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "3              6  2025-09-10T18-37-00Z  After Expire Snapshots  \n",
       "4              2  2025-09-10T18-37-00Z  After Expire Snapshots  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL demo.system.rewrite_data_files('default.pii_data')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File Summary (After Rewrite Data Files) ---\n",
      "Using table name for better reliability...\n",
      "\n",
      "Metadata file summary:\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix  |file_type           |file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|metadata|manifests           |avro       |NULL               |1            |2025-09-10T18-37-01Z|After Rewrite Data Files|\n",
      "|metadata|manifests           |avro       |2025-09-10 18:36:00|2            |2025-09-10T18-37-01Z|After Rewrite Data Files|\n",
      "|metadata|metadata_log_entries|json       |2025-09-10 18:36:00|6            |2025-09-10T18-37-01Z|After Rewrite Data Files|\n",
      "|metadata|snapshots           |avro       |2025-09-10 18:36:00|1            |2025-09-10T18-37-01Z|After Rewrite Data Files|\n",
      "+--------+--------------------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "Data file summary:\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|prefix|file_type|file_format|created_minute     |files_created|run_id              |operation               |\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "|data  |data     |parquet    |2025-09-10 18:36:00|2            |2025-09-10T18-37-01Z|After Rewrite Data Files|\n",
      "+------+---------+-----------+-------------------+-------------+--------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>files_created</th>\n",
       "      <th>run_id</th>\n",
       "      <th>operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-01Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-10T18-37-01Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-01Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-10T18-37-01Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-10T18-37-01Z</td>\n",
       "      <td>After Rewrite Data Files</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro                 NaT   \n",
       "2  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "3  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "4      data                  data     parquet 2025-09-10 18:36:00   \n",
       "\n",
       "   files_created                run_id                 operation  \n",
       "0              1  2025-09-10T18-37-01Z  After Rewrite Data Files  \n",
       "1              1  2025-09-10T18-37-01Z  After Rewrite Data Files  \n",
       "2              2  2025-09-10T18-37-01Z  After Rewrite Data Files  \n",
       "3              6  2025-09-10T18-37-01Z  After Rewrite Data Files  \n",
       "4              2  2025-09-10T18-37-01Z  After Rewrite Data Files  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_previous = all_current.copy(deep=True)\n",
    "_, _, all_current = summarize_files(spark, table_base_path, \"After Rewrite Data Files\")\n",
    "all_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>file_type</th>\n",
       "      <th>file_format</th>\n",
       "      <th>created_minute</th>\n",
       "      <th>minute_str</th>\n",
       "      <th>old_count</th>\n",
       "      <th>new_count</th>\n",
       "      <th>delta</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>data</td>\n",
       "      <td>parquet</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metadata</td>\n",
       "      <td>manifests</td>\n",
       "      <td>avro</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metadata</td>\n",
       "      <td>metadata_log_entries</td>\n",
       "      <td>json</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metadata</td>\n",
       "      <td>snapshots</td>\n",
       "      <td>avro</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>2025-09-10 18:36:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UNCHANGED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prefix             file_type file_format      created_minute  \\\n",
       "0      data                  data     parquet 2025-09-10 18:36:00   \n",
       "1  metadata             manifests        avro 2025-09-10 18:36:00   \n",
       "2  metadata             manifests        avro                 NaT   \n",
       "3  metadata  metadata_log_entries        json 2025-09-10 18:36:00   \n",
       "4  metadata             snapshots        avro 2025-09-10 18:36:00   \n",
       "\n",
       "            minute_str  old_count  new_count  delta     status  \n",
       "0  2025-09-10 18:36:00          2          2      0  UNCHANGED  \n",
       "1  2025-09-10 18:36:00          2          2      0  UNCHANGED  \n",
       "2                  NaN          1          1      0  UNCHANGED  \n",
       "3  2025-09-10 18:36:00          6          6      0  UNCHANGED  \n",
       "4  2025-09-10 18:36:00          1          1      0  UNCHANGED  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare\n",
    "diff = diff_summaries(all_previous, all_current)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to time travel back to the first snapshot. This should fail because the snapshot no longer exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully prevented time travel!\n",
      "Cannot find snapshot with ID 5013975016408165402\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.read.option(\"snapshot-id\", first_snapshot_id).table(\"demo.default.pii_data\").show()\n",
    "except Exception as e:\n",
    "    print(\"Successfully prevented time travel!\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Manual Parquet File Reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section adds a utility to upload a Parquet file from your local computer and display its contents. This is useful for inspecting individual data files, for example, if you download a file from the MinIO bucket to your machine and want to see what's inside to confirm that PII has been physically removed from the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (17.0.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.34.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for the uploader widget and Parquet reader\n",
    "%pip install ipywidgets pandas pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a76881464d94587bebdc304febcd960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.parquet', description='Upload Parquet File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319232026ca2413288bc501aecc830e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import io\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create a file upload widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.parquet',\n",
    "    description='Upload Parquet File',\n",
    "    multiple=False\n",
    ")\n",
    "\n",
    "# Create an output widget to display the DataFrame\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"\n",
    "    This function is triggered when a file is uploaded.\n",
    "    It reads the Parquet file and displays it as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not uploader.value:\n",
    "        return\n",
    "        \n",
    "    # Get the uploaded file info\n",
    "    uploaded_file = uploader.value[0]\n",
    "    file_content = uploaded_file['content']\n",
    "    \n",
    "    # Read the Parquet file content into a Pandas DataFrame\n",
    "    df = pd.read_parquet(io.BytesIO(file_content))\n",
    "    \n",
    "    # Display the DataFrame in the output widget\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Contents of {uploaded_file['name']}:\")\n",
    "        display(df)\n",
    "        \n",
    "    # Reset the uploader so the same file can be uploaded again if needed\n",
    "    uploader.value.clear()\n",
    "    uploader._counter = 0\n",
    "\n",
    "\n",
    "# Observe changes in the uploader's value\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "# Display the uploader and the output area\n",
    "display(uploader, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that we have successfully and permanently deleted the PII from our Iceberg table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "|case_id|first_name|       email_address|key_nm|   secure_txt|  secure_key|update_date|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "| case-1|      NULL|                NULL|  key1|         NULL|secret_key_1| 2023-01-01|\n",
      "| case-2|      Jane|jane.doe@example.com|  key2|secret text 2|secret_key_2| 2023-01-02|\n",
      "+-------+----------+--------------------+------+-------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(f\"\"\"SELECT * FROM demo.default.pii_data TIMESTAMP AS OF '2026-09-02 09:40:00'\"\"\");\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
